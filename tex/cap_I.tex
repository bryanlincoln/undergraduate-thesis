
\chapter{Introdução}
\label{cap:intro}

Desde os últimos anos, algoritmos de Inteligência Artificial (IA) vêm sendo aplicados com grande sucesso em diversos setores da indústria e da sociedade. Suas soluções resolvem problemas que variam desde a detecção, classificação e acompanhamento de doenças \cite{dldiabetic, dlalzheimer} a segmentação de ruas, carros e pedestres para navegação autônoma \cite{pan2017agile, autonomousvehicle}. Aplicações como essas não só permitem a solução de problemas antes inviáveis de se atacar como também permitem a substituição de processos caros e demorados.

Junto dos avanços da IA, avanços na robótica permitiram sua aplicação em tarefas de alta precisão que variam desde impressões 3D a execução de acrobacias \cite{atlaspaper}. Além disso, manipuladores robóticos têm se tornado quase omnipresentes em linhas de produção de indústrias em todo o mundo \cite{automatedassembly}. Em contrapartida, as técnicas de controle mais utilizadas nessas tarefas envolvem o desenvolvimento de modelos matemáticos complexos que devem ser precisamente construídos. Esses modelos levam em consideração, por exemplo, as características físicas do equipamento (e.g. distribuição de massa e torque máximo dos atuadores) e do ambiente (e.g. atrito com o chão, gravidade e rotação da Terra) \cite{coriolis}. A criação desses modelos pode se mostrar uma tarefa consideravelmente difícil e cara para ser executada em larga escala, principalmente quando o equipamento utilizado é formado por numerosas partes móveis e seu ambiente de trabalho é irregular.

Boa parte dos maiores desafios de implementação de robôs com comportamentos complexos são desafios de \textit{software} \cite{dlrobotics}. Especialistas em controle e automação dedicam tempo considerável programando as ações dos atuadores robóticos para que estes executem suas tarefas de forma satisfatória. Outro problema é a falta de generalização de um comportamento entre diferentes equipamentos. Um comportamento programado para um hardware e estrutura específicos é dificilmente portado para outro sem diversos ajustes finos.

Pesquisas recentes em técnicas de IA aplicada a robótica, em especial de aprendizado por reforço, revelaram-se eficazes em substituir a necessidade de comportamentos estritamente programados para execução de tarefas \cite{openaihand, rlindustrial}. Essas técnicas são capazes de aprender comportamentos do zero em simulação de forma que seja possível portá-los para o mundo real mantendo sua robustez a adversidades do ambiente \cite{Tobin_2017}. Além disso, técnicas de aprendizado que trabalham em conjunto de técnicas clássicas a fim de permitir o porte de um comportamento de um equipamento para outro também foram desenvolvidas \cite{rlforcontrol}, permitindo a adaptação do agente para diferentes condições físicas.

Agentes de aprendizado por reforço trabalham com base nos sinais de retorno dados pelo ambiente em que são inseridos. Um problema que surge, com isso, é a necessidade de que os sinais, ou recompensas, guiem o aprendizado para o objetivo esperado. Mesmo sendo um processo menos trabalhoso que a construção de modelos de cinemática inversa, comportamentos complexos como locomoção e manipulação de objetos requerem a modelagem de sinais de recompensa que levam em consideração diversos fatores que podem ser difíceis de se descobrir e balancear \cite{dexterity, ng, deepLoco}. Além disso, frequentemente agentes de aprendizado por reforço encontram problemas no sinal de recompensa modelado que os permitem maximizar seu valor acumulado sem executar a tarefa originalmente proposta \cite{rewardhacking, rewardfaulty}.

Naturalmente, o sinal de recompensa mais simples em tarefas do mundo real diz apenas se um objetivo foi alcançado ou não, ou seja, a recompensa é esparsa em relação as ações tomadas pelo algoritmo. Algoritmos convencionais de aprendizado por reforço tendem a ter dificuldades em aprender em configurações desse tipo \cite{hindsight}. Como alternativa, as noções da psicologia de motivação intrínseca \cite{intrinsicryan} e curiosidade \cite{CuriosityandMotivation} foram aplicadas ao aprendizado por reforço com o objetivo de incentivar a exploração dos agentes em busca de recompensas extrínsecas \cite{curiositylarge, pathak}.

Este trabalho tem como objetivo a aplicação e a avaliação dos conceitos e técnicas de motivação intrínseca para exploração em ambientes de manipulação robótica com recompensa esparsa, dado o sucesso da aplicação destas técnicas em jogos de \textit{Atari}, \textit{Mario} e \textit{VizDoom} \cite{curiositylarge, pathak}. Para isso, este trabalho está organizado da seguinte maneira: no Capítulo \ref{cap:fundamentacao} é apresentada a fundamentação teórica dos conceitos utilizados para a execução dos experimentos; no Capítulo \ref{cap:metodologia} é descrito o ambiente utilizado, o algoritmo proposto e a definição dos testes; no Capítulo \ref{cap:resultados} são apresentados os resultados e no Capítulo \ref{cap:conclusao} são apresentadas as conclusões deste trabalho.