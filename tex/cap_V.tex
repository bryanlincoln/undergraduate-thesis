\chapter{Conclusões}
\label{cap:conclusao}

Neste trabalho, foi apresentada uma alternativa para o treinamento de agentes de aprendizado por reforço em ambientes com recompensa esparsa ou inexistente. Foi demonstrado que agentes utilizando a técnica de motivação intrínseca, ou curiosidade, conseguem resolver tarefas consideravelmente difíceis do domínio da robótica.

No Capítulo \ref{cap:intro} foi exposto o problema da programação de comportamentos complexos em robôs e algumas soluções utilizando técnicas de aprendizado por reforço. Foi apresentado também como comumente estes problemas são formulados para o treinamento de agentes e o desafio do aprendizado em ambientes com recompensa esparsa, que é a motivação deste trabalho. A proposta do uso de um módulo de motivação intrínseca, inspirada por trabalhos recentes na área de aprendizado por reforço, também foi descrita.

No Capítulo \ref{cap:fundamentacao} foram apresentados os fundamentos teóricos que foram base para a preparação e execução dos experimentos. Primeiramente, foram definidos os conceitos de aprendizado de máquina, redes neurais e aprendizado por reforço. Os fundamentos do algoritmo utilizado como base para este trabalho, o PPO, foram apresentados em seguida, assim como o funcionamento do módulo de motivação intrínseca para exploração, que foi o foco deste estudo.

No Capítulo \ref{cap:metodologia} foram definidos os ambientes de \textit{benchmark} selecionados e seu funcionamento foi descrito. O funcionamento completo do algoritmo utilizado e a comunicação entre os modelos de predição de valor do estado, da política e de predição do estado futuro também foram apresentados. Em seguida, a metodologia para os experimentos executados foi definida. Para a demonstração do impacto do módulo de curiosidade no aprendizado, o mesmo algoritmo com os mesmos hiperparâmetros foi executado duas vezes em cada ambiente - uma vez com o módulo de curiosidade e outra vez sem o mesmo.

Os resultados, apresentados no Capítulo \ref{cap:resultados}, demonstram um comportamento exploratório capaz de executar complexas sequências de ações por parte do algoritmo proposto, permitindo-o alcançar o objetivo em todos os ambientes em que foi testado. Os gráficos apresentados demonstram também a tendência do agente a manter políticas exploratórias mesmo alcançando o objetivo geral de cada tarefa proposta. Essas tendências indicam um comportamento possivelmente generalista que pode ser útil para aplicações em problemas que requerem uma abordagem multitarefa ou até de meta-aprendizado.

Nesse contexto, são propostas algumas direções para trabalhos futuros. Primeiramente, o impacto da motivação intrínseca em ambientes multitarefa e de meta-aprendizado pode ser analisado. \textit{Benchmarks} propostos recentemente \cite{metaworld} se mostraram robustos para a avaliação dessas capacidades. Além disso, o estudo da capacidade de generalização do algoritmo para ambientes de teste diferentes dos quais o agente foi treinado é uma importante métrica a ser avaliada. Outras ideias para futuros trabalhos envolvem a aplicação de motivação intrínseca para controle em sistemas complexos e de característica hierárquica, assim como sua aplicação em sistemas de aprendizado multi-agente. Por fim, o uso de curiosidade para aprendizado de controle a partir de píxeis pode ser explorado, dado o sucesso da aplicação desta configuração em jogos \cite{curiositylarge} e pela facilidade e viabilidade do uso de câmeras em robôs reais.